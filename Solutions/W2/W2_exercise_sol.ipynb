{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Background subtraction\n",
    "\n",
    "This python notebook includes all the exercises from this session. All the needed setup, I/O and user-interaction code is already provided to let you concentrate on this week's topic.<br>\n",
    "Please run all the provided code in order before running yours.<br>\n",
    "\n",
    "This week's assignment:\n",
    "\n",
    "## 1. Bluescreen\n",
    "### 1.A Single pixel model\n",
    "Extract foreground objects in bluescreen movie footage. The blue background color should be specified by a single color value [r, g, b]. You may obtain such a value by using the provided notebook widget to select one or more blue pixels on the first frame of the footage. To determine whether a given pixel should be foreground or background, threshold the absolute distance of its value to the reference value, i.e.\n",
    "\n",
    "$$\\left\\lVert[r, g, b] âˆ’ [r_0, g_0, b_0]\\right\\rVert < t$$\n",
    "\n",
    "Based on pixelwise decisions, a mask can be created to specify foreground and background.<br>\n",
    "Try different values for the threshold and the reference color. To test your results, you can use the short video provided. Note that not the whole background consists of bluescreen.\n",
    "To overcome this problem, you can use the precomputed mask provided into the variable `mask`.\n",
    "\n",
    "### 1.B Exemplar set model\n",
    "To further improve the results, specify the blue background by an exemplar set of blue background pixels. You may select this set using the same widget as in the previous point. After computing mean $\\mu$ and covariance matrix $\\Sigma$ of the pixel values, the Mahalanobis distance $M(x, \\mu, \\Sigma)$ can be used to decide whether a color value $x$ originated from the background or the foreground.\n",
    "\n",
    "$$M(x, \\mu, \\Sigma) = \\sqrt{(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$$\n",
    "\n",
    "## 2. Per-pixel Model\n",
    "Probably you have noticed that the approach in the previous task failed in the upper region of the background, which was not covered with blue sheets, for which we provided the `mask`. To handle more complex scenarios, extend the background model to a per-pixel model. For each pixel, compute the mean and the covariance of its values over a number of frames which do not contain foreground objects. You may then use the Mahalanobis distance to classify pixels in sequences containing foreground objects. You can use the same threshold for all pixels.<br>\n",
    "To create the model, use jugglingBG.avi. This scene was captured for several frames without foreground objects. Based on the background model, foreground pixels can then be identified and masks can be created for jugglingTest.avi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information for this tutorial:\n",
    "\n",
    "Images are represented as numpy arrays, which are fixed-dimensional arrays. Videos are represented as arrays of images.<br>\n",
    "In this tutorial each pixel takes a RGB value within the range [0-255] and is represented as a 16-bit signed integer, to allow signed operations without subtle overflow bugs.\n",
    "\n",
    "you will need to visualize your results, thus to printout an image use:\n",
    "\n",
    "`imshow(image)`\n",
    "\n",
    "You can read pixel values inside an image by accessing the corresponding pixel coordinates in the array. The following reads the green component (1) of pixel at location (10, 20) of the 5th frame (index 4):\n",
    "\n",
    "`g = vid[4, 10, 20, 1]`\n",
    "\n",
    "Of course you can also change pixel values in the same way. We recommend copying an image before editing. To copy an image:\n",
    "\n",
    "`im_copy = np.array(im)` (`np` is the short name for `numpy` which is recognized by python when importing numpy with `import numpy as np`)\n",
    "\n",
    "With numpy it is also possible to refer to slices of a given array by specifying the kept dimensions in the slice with `:`. For example, if we want to access the red channel of an RGB image `im` with shape `(height, width, 3)`, we can write `im[:, :, 0]`, which refers to the whole image data for the red channel only. We can optionally specify a range to be kept along a direction by writing the extreme indices around the `:` as in `min:max`. For example, accessing a patch within the pixel coordinates `(h0, w0)`, `(h1, w1)` with h0 < h1 and w0 < w1 can be done with `im[h0:h1, w0:w1, :]`.<br>\n",
    "You can also index arrays based on conditions: for example to set to black all pixels with a blue component higher than 200 you can write `im[im[:, :, 2] > 200] = 0`.<br>\n",
    "These references can be used both for reading an image slice and for writing a whole slice at once, without looping over it.\n",
    "\n",
    "When doing operations between numpy arrays, all operations are executed element-wise (except for the `@` operator that performs the usual matrix multiplication). This requires the shapes (dimensions) of the involved arrays to match or to be compatible according to broadcasting rules, which are usually intuitive. An example of broadcasting is when adding a pixel value `p=np.array([70, 128, 255])` to an image `im` with shape `(height, width, 3)`: the operation is legal and the pixel value is added independently to each pixel in the image. A complete reference of numpy broadcasting rules (not necessary for this tutorial, but very useful for numpy python programming) can be found in https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html . \n",
    "\n",
    "In the following we list some useful functions:\n",
    "\n",
    "`np.array(a)`: returns a copy of `a` as a numpy array. Argument `a` does not need to be a numpy array, it can be any array-like object, e.g. a python list. https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html<br>\n",
    "`np.empty(shape, dtype)`: returns a new empty array with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.empty.html<br>\n",
    "`np.ones(shape, dtype)`: returns a new array filled with ones with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html<br>\n",
    "`np.zeros(shape, dtype)`: returns a new array filled with zeros with specified shape `shape` and the specified data type `dtype`. For this tutorial we suggest using as dtype `np.int16` for images and default for the rest. https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html<br>\n",
    "`np.eye(N)` : builds the identity matrix of order `N`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html<br>\n",
    "`np.reshape(a, newshape)`: returns a reshaped view of the input array `a` having shape `newshape`, which should be specified as a tuple of integers. https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html<br>\n",
    "`np.linalg.norm(x)`: returns the L2 norm of array `x`. You can also specify a keyword argument `axis` to compute the norm only along a specific dimension (the result will have the same shape as `x`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html<br>\n",
    "`np.transpose(a)` : returns the transposed of array `a`, whose shape has reversed order with respect to `a`. Alias `a.T`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html<br>\n",
    "`np.linalg.inv(a)` : returns the multiplicative inverse of matrix `a`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html<br>\n",
    "`np.matmul(a, b)` : returns the matrix multiplication of `a` multiplied by `b`. Alias `a @ b`. https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html<br>\n",
    "`np.sum(a)` : returns the sum of all the elements of array `a`. You can specify a keyword argument `axis` to compute the sum only along a specific dimension (the result will have the same shape as `a`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html<br>\n",
    "`np.mean(a)` : returns the mean value of all the elements of array `a`. You can specify a keyword argument `axis` to compute the mean only along a specific dimension (the result will have the same shape as `a`, but without the specified dimension) https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html<br>\n",
    "`np.cov(m)` : build the covariance matrix of array `a` where each row represents a variable and each column represents an observation.  https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1 - SETUP CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell installs required packages into the virtual machine that will run your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: scikit-video in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (1.1.11)\n",
      "Requirement already satisfied: pillow in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-video) (6.2.0)\n",
      "Requirement already satisfied: scipy in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-video) (1.2.2)\n",
      "Requirement already satisfied: numpy in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-video) (1.16.5)\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: scikit-image in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (0.14.5)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (1.2.2)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (1.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (1.12.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (1.0.3)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scikit-image) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from scipy>=0.17.0->scikit-image) (1.16.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from networkx>=1.8->scikit-image) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.4.2)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.5)\n",
      "Requirement already satisfied: subprocess32 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (3.5.4)\n",
      "Requirement already satisfied: pytz in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /home/lucas/.virtualenvs/vis_comp_env/lib/python2.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-video\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages to read videos, make interactive widgets and plot images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "from ipywidgets import interactive, widgets\n",
    "from IPython.display import display\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the test video bluescreen.avi in `bluescreen` and the upper region mask into `umask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video information:\n",
      "('Number of frames: ', 43)\n",
      "('Frame width (pixels): ', 516)\n",
      "('Frame height (pixels): ', 389)\n",
      "('Video array shape: ', (43, 389, 516, 3))\n"
     ]
    }
   ],
   "source": [
    "bluescreen = np.array(skvideo.io.vread(\"bluescreen.avi\"), dtype=np.int16)\n",
    "numframes, height, width, channels = np.shape(bluescreen)\n",
    "print(\"Video information:\")\n",
    "print(\"Number of frames: \", numframes)\n",
    "print(\"Frame width (pixels): \", width)\n",
    "print(\"Frame height (pixels): \", height)\n",
    "print(\"Video array shape: \", np.shape(bluescreen))\n",
    "\n",
    "umask = skimage.io.imread(\"mask.bmp\")//255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a \\*wonderful\\* user interface we have prepared for you to select any number of points on the first frame of the video.\n",
    "\n",
    "Use the x slider to move vertically and use the y slider to move horizontally (x and y reflect indexing in the image array).<br>\n",
    "Press \"Update image\" when you want to see where you moved your cursor within the image, and press \"Remember this point\" when you want to store the current selected point into the python list `selected_image_locations`.\n",
    "\n",
    "\n",
    "`selected_image_locations` will contain all selected locations stored as couples of integers, taking the format: [(x1, y1), (x2, y2), (x3, y3) ...]\n",
    "\n",
    "The image will show a red cross where there is the current temporary selection, and green crosses on all stored locations.\n",
    "\n",
    "If you are not familiar with numpy arrays, we suggest to read through the code of `display_selections` and `draw_cross` as further examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e30f96088540feb0f95a3cf24af1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description=u'Remember this point', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05caa44edd44225ad6d5faccec02c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0xOTQsIGRlc2NyaXB0aW9uPXUneCcsIG1heD0zODgpLCBJbnRTbGlkZXIodmFsdWU9MjU3LCBkZXNjcmlwdGlvbj11J3knLCDigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_image_locations = list()\n",
    "\n",
    "def draw_cross(im, cx, cy, l, w, col):\n",
    "    dimx, dimy = np.shape(im)[:2]\n",
    "    im[max(0, cx-l):min(cx+l, dimx-1), max(0, cy-w):min(dimy-1, cy+w)] = col\n",
    "    im[max(0, cx-w):min(dimx-1, cx+w), max(0, cy-l):min(dimy-1, cy+l)] = col\n",
    "\n",
    "\n",
    "def display_selections(x, y):\n",
    "    tmp_im = np.array(bluescreen[0]*umask)\n",
    "    for px, py in selected_image_locations:\n",
    "        draw_cross(tmp_im, px, py, 1, 5, (0, 255, 0))\n",
    "    draw_cross(tmp_im, x, y, 1, 5, (255, 0, 0))\n",
    "    imshow(tmp_im)\n",
    "    \n",
    "interactive_plot = interactive(display_selections,{'manual': True, 'manual_name': 'Update image'}, x=(0, height-1, 1), y=(0, width-1, 1))\n",
    "button = widgets.Button(description=\"Remember this point\")\n",
    "xslider = interactive_plot.children[0]\n",
    "yslider = interactive_plot.children[1]\n",
    "button.on_click(lambda b: selected_image_locations.append((xslider.value, yslider.value)))\n",
    "display(button)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1.A - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e924cba16c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msely\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_image_locations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mref_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbluescreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msely\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTHRESHOLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "selx, sely = selected_image_locations[0]\n",
    "ref_rgb = bluescreen[0, selx, sely]\n",
    "\n",
    "THRESHOLD = 200\n",
    "\n",
    "def detect_bg(i):\n",
    "    bgmask = np.array(umask)\n",
    "    bgmask[np.linalg.norm(bluescreen[i] - ref_rgb, axis=-1) < THRESHOLD] = 0\n",
    "    return bgmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the quality of the solution here depends on the first chosen point! You can play with selected points and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = widgets.Play(\n",
    "    interval=1000,\n",
    "    min=0,\n",
    "    max=numframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "def print_frame(i):\n",
    "    imshow(bluescreen[i]*detect_bg(i))\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, numframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1.B - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplar_pixel_set = np.array([bluescreen[0, selx, sely] for selx, sely in selected_image_locations])  # the [`expression` for `element` in `iterator`] list constructor is just an equivalent pythonic short-hand for a for-loop\n",
    "mean = np.mean(exemplar_pixel_set, axis=0)\n",
    "inv_cov = np.linalg.inv(np.cov(exemplar_pixel_set.T))\n",
    "\n",
    "THRESHOLD = 10\n",
    "\n",
    "def detect_bg_mah_loop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \n",
    "    This function is perfectly equivalent to detect_bg_mah_noloop, but uses explitic python looping\n",
    "    \"\"\"\n",
    "    bgmask = np.array(umask)\n",
    "    \n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            residual = bluescreen[i, h, w]-mu\n",
    "            if residual.T @ inv_cov @ residual < THRESHOLD**2:\n",
    "                bgmask[h, w] = 0\n",
    "\n",
    "    return bgmask\n",
    "\n",
    "def detect_bg_mah_noloop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \n",
    "    This function is perfectly equivalent to detect_bg_mah_loop, but uses implicit numpy broadcasting rules (gains in performance)\n",
    "    \"\"\"\n",
    "    bgmask = np.array(umask)\n",
    "    \n",
    "    residuals = bluescreen[i]-mean  # precompute all differences from the mean value\n",
    "    distance_image = np.sum((residuals[:, :, np.newaxis, :] @ inv_cov)[:, :, 0, :] * residuals, axis=-1)  # build an image where each pixel is the mahalanobis distance from the corresponding source pixel. See docpage of matmul for details on its specific broadcasting rules.\n",
    "    \n",
    "    bgmask[distance_image < THRESHOLD**2] = 0  # conditional assignment of mask values based on the precomputed distance image.\n",
    "    return bgmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the quality of the solution here depends on the chosen points! You can play with selected points and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = widgets.Play(\n",
    "    interval=1000,\n",
    "    min=0,\n",
    "    max=numframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "def print_frame(i):\n",
    "    imshow(bluescreen[i]*detect_bg_mah_noloop(i))\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, numframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2 - SETUP CODE\n",
    "(installs and imports from the Exercise 1 should be run anyway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the test video jugglingTest.avi in `testvid` and the background reference video jugglingBG.avi into `bgvid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jugglingBG.avi video information:\n",
      "('Number of frames: ', 20)\n",
      "('Frame width (pixels): ', 672)\n",
      "('Frame height (pixels): ', 378)\n",
      "('Video array shape: ', (20, 378, 672, 3))\n",
      "()\n",
      "jugglingTest.avi video information:\n",
      "('Number of frames: ', 100)\n",
      "('Frame width (pixels): ', 672)\n",
      "('Frame height (pixels): ', 378)\n",
      "('Video array shape: ', (100, 378, 672, 3))\n"
     ]
    }
   ],
   "source": [
    "bgvid = np.array(skvideo.io.vread(\"jugglingBG.avi\"), dtype=np.int16)\n",
    "testvid = np.array(skvideo.io.vread(\"jugglingTest.avi\"), dtype=np.int16)\n",
    "\n",
    "bgnumframes, bgheight, bgwidth, bgchannels = np.shape(bgvid)\n",
    "print(\"jugglingBG.avi video information:\")\n",
    "print(\"Number of frames: \", bgnumframes)\n",
    "print(\"Frame width (pixels): \", bgwidth)\n",
    "print(\"Frame height (pixels): \", bgheight)\n",
    "print(\"Video array shape: \", np.shape(bgvid))\n",
    "\n",
    "print()\n",
    "testnumframes, testheight, testwidth, testchannels = np.shape(testvid)\n",
    "print(\"jugglingTest.avi video information:\")\n",
    "print(\"Number of frames: \", testnumframes)\n",
    "print(\"Frame width (pixels): \", testwidth)\n",
    "print(\"Frame height (pixels): \", testheight)\n",
    "print(\"Video array shape: \", np.shape(testvid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2 - PUT SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the inverse covariance matrix for every pixel is going to require some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9591d764b5d9417b9a3530875c18389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=672)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "means = np.mean(bgvid, axis=0) # extract the mean value for each pixel by averaging over subsequent frames\n",
    "inv_cov_matrices = np.empty((bgheight, bgwidth, 3, 3))\n",
    "\n",
    "progress_bar = widgets.IntProgress(min=0.0, max=bgwidth, description='')\n",
    "display(progress_bar)\n",
    "for w in range(bgwidth):\n",
    "    for h in range(bgheight):\n",
    "        inv_cov_matrices[h, w] = np.linalg.inv(np.cov(bgvid[:, h, w, :].T) + 1e-8*np.eye(3))\n",
    "    progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 80\n",
    "\n",
    "def detect_bg_perpixelmodel_loop(i):\n",
    "    \"\"\"\n",
    "    Compute the background mask using the mahalanobis distance.\n",
    "    \"\"\"\n",
    "    bgmask = np.ones(shape=(testheight, testwidth, 1), dtype=np.int16)\n",
    "    \n",
    "    for h in range(testheight):\n",
    "        for w in range(testwidth):\n",
    "            residual = testvid[i, h, w]-means[h, w]\n",
    "            if residual.T @ inv_cov_matrices[h, w] @ residual < THRESHOLD**2:\n",
    "                bgmask[h, w] = 0\n",
    "\n",
    "    return bgmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = widgets.Play(\n",
    "    interval=3000,\n",
    "    min=0,\n",
    "    max=numframes-1,\n",
    "    step=1,\n",
    ")\n",
    "\n",
    "def print_frame(i):\n",
    "    imshow(testvid[i]*detect_bg_perpixelmodel_loop(i))\n",
    "    \n",
    "slider = interactive(print_frame, i=(0, testnumframes-1, 1))\n",
    "widgets.jslink((play, 'value'), (slider.children[0], 'value'))\n",
    "widgets.VBox([play, slider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
